{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRpsy5Zp8Lr"
   },
   "source": [
    "\n",
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "\n",
    "#### <font color=red>For lab assignment, you will work with two datasets. The trained weights need to be saved and shared with us in a folder called models with the name ./models/{dataset_name}_weights.pkl. Your predict function should load these weights, initialize the DNN and predict the labels.</font>\n",
    "\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between \n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "id": "mBBZWQn3WjsN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9j3in3odIle"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "    \"\"\"\n",
    "    Implement Normalization for input image features\n",
    "    Args:\n",
    "        X : numpy array of shape (n_samples, 784)\n",
    "    Returns:\n",
    "        X_out: numpy array of shape (n_samples, 784) after normalization\n",
    "    \"\"\"\n",
    "    X_out = None\n",
    "    ## TODO\n",
    "    X_out = np.zeros(X.shape, dtype = np.float64)\n",
    "    ind = (np.std(X, axis = 0)!=0)\n",
    "    X_out[:, ind] = X[:, ind]\n",
    "    X_out[:,ind] = (X_out[:, ind] - np.mean(X_out[:, ind], axis = 0))/(np.std(X_out[:, ind], axis = 0))\n",
    "    X_out[:, ind] = (X_out[:, ind] - X_out[:, ind].min(axis = 0))/(\n",
    "        X_out[:, ind].max(axis = 0) - X_out[:, ind].min(axis = 0))\n",
    "    ## END TODO\n",
    "\n",
    "    assert X_out.shape == X.shape\n",
    "\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejROq-52YUol"
   },
   "source": [
    "### Split data into train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "id": "l07sJgZ3XG-N"
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_ratio=0.8):\n",
    "    '''\n",
    "    Split data into train and validation sets\n",
    "    The first floor(train_ratio*n_sample) samples form the train set\n",
    "    and the remaining the test set\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    train_ratio - fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "    X_train, Y_train, X_val, Y_val\n",
    "    '''\n",
    "    # Try Normalization and scaling and store it in X_transformed\n",
    "    X_transformed = X\n",
    "\n",
    "    ## TODO\n",
    "    X_transformed = preprocessing(X)\n",
    "    ## END TODO\n",
    "\n",
    "    assert X_transformed.shape == X.shape\n",
    "\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    num_train_samples = math.floor(num_samples * train_ratio)\n",
    "    train_indices = np.random.choice(indices, num_train_samples, replace=False)\n",
    "    val_indices = list(set(indices) - set(train_indices))\n",
    "    X_train, Y_train, X_val, Y_val = X_transformed[train_indices], Y[train_indices], X_transformed[val_indices], Y[val_indices]\n",
    "  \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FujjCCbMbsu4"
   },
   "source": [
    "#Flatten the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "id": "hl8LxP1lAEiN"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    '''\n",
    "    This class converts a multi-dimensional into 1-d vector\n",
    "    '''\n",
    "    def __init__(self, input_shape):\n",
    "        '''\n",
    "        Args:\n",
    "        input_shape : Original shape, tuple of ints\n",
    "        '''\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Flatten\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Converts a multi-dimensional into 1-d vector\n",
    "        Args:\n",
    "          input : training data, numpy array of shape (n_samples , self.input_shape)\n",
    "\n",
    "        Returns:\n",
    "          input: training data, numpy array of shape (n_samples , -1)\n",
    "        '''\n",
    "        ## TODO\n",
    "        n_samples = input.shape[0]\n",
    "        return input.reshape(n_samples, -1)\n",
    "        ## END TODO\n",
    "        \n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Converts back the passed array to original dimention \n",
    "        Args:\n",
    "        output_error :  numpy array \n",
    "        learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "        output_error: A reshaped numpy array to allow backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        return output_error.reshape(-1,self.input_shape[0], self.input_shape[1])\n",
    "        ## END TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02MOHEdgh7T6"
   },
   "source": [
    "#Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "id": "oTrTMpTwtLXd"
   },
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    '''\n",
    "    Implements a fully connected layer  \n",
    "    '''\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Args:\n",
    "         input_size : Input shape, int\n",
    "         output_size: Output shape, int \n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        ## TODO\n",
    "        np.random.seed(0)\n",
    "        self.weights = np.random.random((input_size,output_size)) * 2 + -1\n",
    "        self.bias = np.random.random((1, output_size)) *2 + -1\n",
    "        self.weights/= input_size\n",
    "        self.bias/= input_size\n",
    "        ## END TODO\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Fully connected\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of a fully connected network\n",
    "        Args:\n",
    "          input : training data, numpy array of shape (n_samples , self.input_size)\n",
    "\n",
    "        Returns:\n",
    "           numpy array of shape (n_samples , self.output_size)\n",
    "        '''\n",
    "        ## TODO\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "        ## END TODO\n",
    "        \n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter \n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        w_grad = np.dot(np.transpose(self.input), output_error)\n",
    "        b_grad = output_error.sum(axis = 0)\n",
    "#         print(output_error.shape)\n",
    "        out_grad = np.dot(output_error, np.transpose(self.weights))\n",
    "        self.weights -= learning_rate * w_grad\n",
    "        self.bias -= learning_rate * b_grad\n",
    "        return out_grad\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "id": "E6nSYAB2sam3"
   },
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    '''\n",
    "    Implements a Activation layer which applies activation function on the inputs. \n",
    "    '''\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        '''\n",
    "          Args:\n",
    "          activation : Name of the activation function (sigmoid,tanh or relu)\n",
    "          activation_prime: Name of the corresponding function to be used during backpropagation (sigmoid_prime,tanh_prime or relu_prime)\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Activation\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Applies the activation function \n",
    "        Args:\n",
    "          input : numpy array on which activation function is to be applied\n",
    "\n",
    "        Returns:\n",
    "           numpy array output from the activation function\n",
    "        '''\n",
    "        ## TODO\n",
    "        self.output = self.activation(input)\n",
    "        return self.output\n",
    "        ## END TODO\n",
    "        \n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter \n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        return output_error * self.activation_prime(self.output)\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "id": "RQeuIfkK3vyl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SoftmaxLayer:\n",
    "    '''\n",
    "      Implements a Softmax layer which applies softmax function on the inputs. \n",
    "    '''\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Softmax\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Applies the softmax function \n",
    "        Args:\n",
    "          input : numpy array on which softmax function is to be applied\n",
    "\n",
    "        Returns:\n",
    "           numpy array output from the softmax function\n",
    "        '''\n",
    "        ## TODO\n",
    "#         try:\n",
    "        temp = np.exp(input)\n",
    "#         except:\n",
    "#         print(input.max(), input.min())\n",
    "        self.output = temp/(np.sum(temp, axis = 1).reshape(-1,1))\n",
    "        return self.output\n",
    "        ## END TODO\n",
    "        \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a Softmax layer\n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        temp = output_error.copy()\n",
    "        temp += (-1 * output_error * self.output).sum(axis = 1).reshape(-1,1)\n",
    "        answer = self.output * temp\n",
    "        return answer\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {
    "id": "LuPbn70Wt8Q7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Sigmoid function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying simoid function\n",
    "    '''\n",
    "    ## TODO\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    ## END TODO\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    '''\n",
    "     Implements derivative of Sigmoid function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of Sigmoid function\n",
    "    '''\n",
    "    ## TODO\n",
    "    temp = sigmoid(x)\n",
    "    return temp * (1 - temp)\n",
    "    ## END TODO\n",
    "\n",
    "def tanh(x):\n",
    "    '''\n",
    "    Tanh function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying tanh function\n",
    "    '''\n",
    "    ## TODO\n",
    "    temp_exp = np.exp(2 * x)\n",
    "    return (temp_exp - 1)/(temp_exp + 1)\n",
    "    ## END TODO\n",
    "\n",
    "def tanh_prime(x):\n",
    "    '''\n",
    "     Implements derivative of Tanh function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of Tanh function\n",
    "    '''\n",
    "    ## TODO\n",
    "    return 1 - (tanh(x)**2)\n",
    "    ## END TODO\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    ReLU function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying ReLU function\n",
    "    '''\n",
    "    ## TODO\n",
    "    temp = x.copy()\n",
    "    temp[temp<0] = 0\n",
    "    return temp\n",
    "    ## END TODO\n",
    "\n",
    "def relu_prime(x):\n",
    "    '''\n",
    "     Implements derivative of ReLU function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of ReLU function\n",
    "    '''\n",
    "    ## TODO\n",
    "    temp = np.zeros(x.shape, dtype = np.float64)\n",
    "    temp[x>0] = 1\n",
    "    return temp\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {
    "id": "rXY7jkUzuqEk"
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "    MSE loss\n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array \n",
    "        y_true :  Predicted labels, numpy array \n",
    "    Returns:\n",
    "       loss : float\n",
    "    '''\n",
    "    ## TODO\n",
    "    num_samples = y_true.shape[0]\n",
    "    mean_err = np.mean((1 - y_pred[np.arange(num_samples), y_true])**2)\n",
    "    return mean_err\n",
    "    ## END TODO\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    '''\n",
    "    Implements derivative of MSE function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of MSE function\n",
    "    '''\n",
    "    ## TODO\n",
    "    grad = np.zeros(y_pred.shape)\n",
    "    num_samples = y_true.shape[0]\n",
    "    grad[np.arange(num_samples), y_true] = 2 * (y_pred[np.arange(num_samples), y_true] - 1)\n",
    "    return grad\n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "    Cross entropy loss \n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array \n",
    "        y_true :  Predicted labels, numpy array \n",
    "    Returns:\n",
    "       loss : float\n",
    "    '''\n",
    "    ## TODO\n",
    "    num_samples = y_true.shape[0]\n",
    "    vec = np.log(y_pred[np.arange(num_samples), y_true])\n",
    "    return np.mean(vec) * -1\n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred):\n",
    "    '''\n",
    "    Implements derivative of cross entropy function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of cross entropy function\n",
    "    '''\n",
    "    ## TODO\n",
    "    grad = np.zeros(y_pred.shape)\n",
    "    num_samples = y_true.shape[0]\n",
    "    grad[np.arange(num_samples), y_true] = -1/(y_pred[np.arange(num_samples), y_true])\n",
    "    return grad\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u23euUDztNtb"
   },
   "source": [
    "Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "id": "-sCYdGN8tSdp"
   },
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train,dataset_name):\n",
    "\n",
    "    '''\n",
    "    Create and trains a feedforward network\n",
    "\n",
    "    Do not forget to save the final weights of the feed forward network to a file. Use these weights in the `predict` function \n",
    "    Args:\n",
    "        X_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        Y_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        dataset_name -- name of the dataset (flowers or mnist)\n",
    "    \n",
    "    '''\n",
    "     \n",
    "    #Note that this just a template to help you create your own feed forward network \n",
    "    ## TODO\n",
    "\n",
    "    #define your network\n",
    "    #This network would work only for mnist\n",
    "    if(dataset_name == \"mnist\"):\n",
    "        epochs = 30\n",
    "        network = [\n",
    "            FlattenLayer(input_shape=(28, 28)),\n",
    "            FCLayer(28 * 28, 30),\n",
    "            ActivationLayer(relu, relu_prime),\n",
    "            FCLayer(30, 15),\n",
    "            ActivationLayer(relu, relu_prime),\n",
    "            FCLayer(15, 10),\n",
    "            SoftmaxLayer(10)\n",
    "        ]\n",
    "        train_X, train_Y, val_X, val_Y = split_data(X_train, Y_train, 0.75)\n",
    "        n_samples = train_X.shape[0]\n",
    "        batch_size = n_samples//50\n",
    "        learning_rate = 0.99/batch_size\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        epochs = 20\n",
    "        train_X, train_Y, val_X, val_Y = split_data(X_train, Y_train, 0.8)\n",
    "        network = [\n",
    "            FCLayer(2048, 5),\n",
    "            ActivationLayer(relu, relu_prime),\n",
    "            FCLayer(5, 5),\n",
    "            SoftmaxLayer(5)\n",
    "        ]\n",
    "        \n",
    "        n_samples = train_X.shape[0]\n",
    "        batch_size = 50\n",
    "        learning_rate = 0.1/batch_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        for start_pt in range(0, n_samples, batch_size):\n",
    "            end_pt = min(start_pt + batch_size , n_samples)\n",
    "            x = train_X[start_pt:end_pt, :]\n",
    "            y_true = train_Y[start_pt:end_pt]\n",
    "            # forward\n",
    "            output = x\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "            \n",
    "            # error (display purpose only)\n",
    "            err =  cross_entropy(y_true, output)\n",
    "            error += err * (end_pt - start_pt)\n",
    "\n",
    "            # backward\n",
    "            output_error = cross_entropy_prime(y_true, output)\n",
    "            for layer in reversed(network):\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "        \n",
    "        error /= len(train_X)\n",
    "        print('%d/%d, error=%f' % (epoch + 1, epochs, error))\n",
    "        output_train = train_X\n",
    "        output_val = val_X\n",
    "        for layer in network:\n",
    "            output_train = layer.forward(output_train)\n",
    "            output_val = layer.forward(output_val)\n",
    "        train_pred = output_train.argmax(axis = 1)\n",
    "        val_pred = output_val.argmax(axis = 1)\n",
    "        accuracy_train = np.sum(train_pred == train_Y)/ len(output_train)\n",
    "        accuracy_val = np.sum(val_pred == val_Y)/len(output_val)\n",
    "        print(f\"Train accuracy = {accuracy_train}, Val_accuracy = {accuracy_val} for dataset = {dataset_name}\")\n",
    "        \n",
    "\n",
    "    #Save you model weights\n",
    "#     if dataset_name == \"mnist\":\n",
    "    with open(f\"./models/{dataset_name}_weights.pkl\", \"wb\") as file:\n",
    "        pkl.dump(network , file)\n",
    "    output_train = train_X\n",
    "    output_val = val_X\n",
    "    for layer in network:\n",
    "        output_train = layer.forward(output_train)\n",
    "        output_val = layer.forward(output_val)\n",
    "    train_pred = output_train.argmax(axis = 1)\n",
    "    val_pred = output_val.argmax(axis = 1)\n",
    "    accuracy_train = np.sum(train_pred == train_Y)/ len(output_train)\n",
    "    accuracy_val = np.sum(val_pred == val_Y)/len(output_val)\n",
    "    print(f\"Train accuracy = {accuracy_train}, Val_accuracy = {accuracy_val} for dataset = {dataset_name}\")\n",
    "    ## END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3Pop_HsvuEZ"
   },
   "source": [
    "Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "id": "ttYbN2psvtu_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x -- (60000, 28, 28); train_y -- (60000,)\n",
      "1/30, error=2.278943\n",
      "Train accuracy = 0.3237111111111111, Val_accuracy = 0.3244 for dataset = mnist\n",
      "2/30, error=1.438012\n",
      "Train accuracy = 0.6466, Val_accuracy = 0.6456 for dataset = mnist\n",
      "3/30, error=0.686754\n",
      "Train accuracy = 0.8760222222222223, Val_accuracy = 0.8757333333333334 for dataset = mnist\n",
      "4/30, error=0.444628\n",
      "Train accuracy = 0.898, Val_accuracy = 0.8922666666666667 for dataset = mnist\n",
      "5/30, error=0.322593\n",
      "Train accuracy = 0.9259111111111111, Val_accuracy = 0.9221333333333334 for dataset = mnist\n",
      "6/30, error=0.265115\n",
      "Train accuracy = 0.9295333333333333, Val_accuracy = 0.9251333333333334 for dataset = mnist\n",
      "7/30, error=0.231877\n",
      "Train accuracy = 0.9327111111111112, Val_accuracy = 0.9264666666666667 for dataset = mnist\n",
      "8/30, error=0.408585\n",
      "Train accuracy = 0.9318222222222222, Val_accuracy = 0.9278 for dataset = mnist\n",
      "9/30, error=0.219991\n",
      "Train accuracy = 0.9374222222222223, Val_accuracy = 0.9326666666666666 for dataset = mnist\n",
      "10/30, error=0.198263\n",
      "Train accuracy = 0.9448888888888889, Val_accuracy = 0.939 for dataset = mnist\n",
      "11/30, error=0.191187\n",
      "Train accuracy = 0.9484888888888889, Val_accuracy = 0.9421333333333334 for dataset = mnist\n",
      "12/30, error=0.182019\n",
      "Train accuracy = 0.9508, Val_accuracy = 0.9429333333333333 for dataset = mnist\n",
      "13/30, error=0.170599\n",
      "Train accuracy = 0.9546, Val_accuracy = 0.9456 for dataset = mnist\n",
      "14/30, error=0.165788\n",
      "Train accuracy = 0.9569111111111112, Val_accuracy = 0.9488666666666666 for dataset = mnist\n",
      "15/30, error=0.154192\n",
      "Train accuracy = 0.9586888888888889, Val_accuracy = 0.9492666666666667 for dataset = mnist\n",
      "16/30, error=0.148425\n",
      "Train accuracy = 0.9600222222222222, Val_accuracy = 0.9501333333333334 for dataset = mnist\n",
      "17/30, error=0.143371\n",
      "Train accuracy = 0.9613777777777778, Val_accuracy = 0.9508666666666666 for dataset = mnist\n",
      "18/30, error=0.140864\n",
      "Train accuracy = 0.9625111111111111, Val_accuracy = 0.9507333333333333 for dataset = mnist\n",
      "19/30, error=0.131770\n",
      "Train accuracy = 0.9637111111111111, Val_accuracy = 0.9508666666666666 for dataset = mnist\n",
      "20/30, error=0.128300\n",
      "Train accuracy = 0.9644222222222222, Val_accuracy = 0.952 for dataset = mnist\n",
      "21/30, error=0.166965\n",
      "Train accuracy = 0.9607333333333333, Val_accuracy = 0.9486 for dataset = mnist\n",
      "22/30, error=0.134263\n",
      "Train accuracy = 0.9577777777777777, Val_accuracy = 0.9448 for dataset = mnist\n",
      "23/30, error=0.127373\n",
      "Train accuracy = 0.9640444444444445, Val_accuracy = 0.9515333333333333 for dataset = mnist\n",
      "24/30, error=0.125450\n",
      "Train accuracy = 0.9661333333333333, Val_accuracy = 0.9516 for dataset = mnist\n",
      "25/30, error=0.121633\n",
      "Train accuracy = 0.9661111111111111, Val_accuracy = 0.9514666666666667 for dataset = mnist\n",
      "26/30, error=0.120775\n",
      "Train accuracy = 0.9669777777777778, Val_accuracy = 0.9516666666666667 for dataset = mnist\n",
      "27/30, error=0.118082\n",
      "Train accuracy = 0.9672888888888889, Val_accuracy = 0.9524 for dataset = mnist\n",
      "28/30, error=0.125824\n",
      "Train accuracy = 0.9671777777777778, Val_accuracy = 0.9523333333333334 for dataset = mnist\n",
      "29/30, error=0.110118\n",
      "Train accuracy = 0.9689777777777778, Val_accuracy = 0.9533333333333334 for dataset = mnist\n",
      "30/30, error=0.109016\n",
      "Train accuracy = 0.9698888888888889, Val_accuracy = 0.9536 for dataset = mnist\n",
      "Train accuracy = 0.9698888888888889, Val_accuracy = 0.9536 for dataset = mnist\n",
      "train_x -- (2936, 2048); train_y -- (2936,)\n",
      "1/20, error=1.481747\n",
      "Train accuracy = 0.502129471890971, Val_accuracy = 0.48299319727891155 for dataset = flowers\n",
      "2/20, error=1.065488\n",
      "Train accuracy = 0.6682282793867121, Val_accuracy = 0.6462585034013606 for dataset = flowers\n",
      "3/20, error=0.830209\n",
      "Train accuracy = 0.7312606473594548, Val_accuracy = 0.6904761904761905 for dataset = flowers\n",
      "4/20, error=0.706742\n",
      "Train accuracy = 0.7674616695059625, Val_accuracy = 0.7278911564625851 for dataset = flowers\n",
      "5/20, error=0.622607\n",
      "Train accuracy = 0.7989778534923339, Val_accuracy = 0.7568027210884354 for dataset = flowers\n",
      "6/20, error=0.559791\n",
      "Train accuracy = 0.823679727427598, Val_accuracy = 0.782312925170068 for dataset = flowers\n",
      "7/20, error=0.508240\n",
      "Train accuracy = 0.8415672913117547, Val_accuracy = 0.7976190476190477 for dataset = flowers\n",
      "8/20, error=0.463512\n",
      "Train accuracy = 0.8615843270868825, Val_accuracy = 0.8112244897959183 for dataset = flowers\n",
      "9/20, error=0.423759\n",
      "Train accuracy = 0.8786201022146508, Val_accuracy = 0.8112244897959183 for dataset = flowers\n",
      "10/20, error=0.388714\n",
      "Train accuracy = 0.8901192504258943, Val_accuracy = 0.8112244897959183 for dataset = flowers\n",
      "11/20, error=0.357489\n",
      "Train accuracy = 0.8943781942078365, Val_accuracy = 0.8214285714285714 for dataset = flowers\n",
      "12/20, error=0.329730\n",
      "Train accuracy = 0.9071550255536627, Val_accuracy = 0.8197278911564626 for dataset = flowers\n",
      "13/20, error=0.304628\n",
      "Train accuracy = 0.9178023850085179, Val_accuracy = 0.8231292517006803 for dataset = flowers\n",
      "14/20, error=0.282156\n",
      "Train accuracy = 0.9250425894378195, Val_accuracy = 0.8282312925170068 for dataset = flowers\n",
      "15/20, error=0.262271\n",
      "Train accuracy = 0.9310051107325383, Val_accuracy = 0.826530612244898 for dataset = flowers\n",
      "16/20, error=0.244397\n",
      "Train accuracy = 0.9378194207836457, Val_accuracy = 0.8248299319727891 for dataset = flowers\n",
      "17/20, error=0.228235\n",
      "Train accuracy = 0.9412265758091993, Val_accuracy = 0.8299319727891157 for dataset = flowers\n",
      "18/20, error=0.213705\n",
      "Train accuracy = 0.9471890971039182, Val_accuracy = 0.8282312925170068 for dataset = flowers\n",
      "19/20, error=0.199597\n",
      "Train accuracy = 0.9514480408858603, Val_accuracy = 0.8299319727891157 for dataset = flowers\n",
      "20/20, error=0.186826\n",
      "Train accuracy = 0.9535775127768313, Val_accuracy = 0.8248299319727891 for dataset = flowers\n",
      "Train accuracy = 0.9535775127768313, Val_accuracy = 0.8248299319727891 for dataset = flowers\n"
     ]
    }
   ],
   "source": [
    "dataset = \"mnist\" \n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_mnist = pkl.load(file)\n",
    "    print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
    "\n",
    "fit(train_mnist[0],train_mnist[1],'mnist')\n",
    "\n",
    "dataset = \"flowers\" # \"mnist\"/\"flowers\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_flowers = pkl.load(file)\n",
    "    print(f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n",
    "\n",
    "fit(train_flowers[0],train_flowers[1],'flowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "id": "QprSHht4iwe9"
   },
   "outputs": [],
   "source": [
    "def predict(X_test, dataset_name):\n",
    "    \"\"\"\n",
    "\n",
    "    X_test -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "\n",
    "\n",
    "\n",
    "    This is the only function that we will call from the auto grader. \n",
    "\n",
    "    This function should only perform inference, please donot train your models here.\n",
    "\n",
    "    Steps to be done here:\n",
    "    1. Load your trained weights from ./models/{dataset_name}_weights.pkl\n",
    "    2. Ensure that you read weights using only the libraries we have given above.\n",
    "    3. Initialize your model with your trained weights\n",
    "    4. Compute the predicted labels and return it\n",
    "\n",
    "    Please provide us the complete code you used for training including any techniques\n",
    "    like data augmentation etc. that you have tried out. \n",
    "\n",
    "    Return:\n",
    "    Y_test - nparray of shape (num_test,)\n",
    "    \"\"\"\n",
    "    Y_test = np.zeros(X_test.shape,)\n",
    "\n",
    "    ## TODO\n",
    "    X_test = preprocessing(X_test)\n",
    "    with open(f\"./models/{dataset_name}_weights.pkl\", \"rb\") as file:\n",
    "        network = pkl.load(file)\n",
    "        output = X_test\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        Y_test = np.argmax(output, axis = 1)\n",
    "#         print(Y_test.shape, X_test.shape)\n",
    "#         print(type(Y_test), type(X_test))\n",
    "\n",
    "    ## END TODO\n",
    "    assert Y_test.shape == (X_test.shape[0],) and type(Y_test) == type(X_test), \"Check what you return\"\n",
    "    return Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9235875, Max = 0.9475, Min = 0.89, dataset = flowers\n",
      "Accuracy = 0.9726725, Max = 0.9785, Min = 0.9665, dataset = mnist\n"
     ]
    }
   ],
   "source": [
    "#check the predict function\n",
    "# for dataset in [\"flowers\",\"mnist\"]:\n",
    "#     accuracy = 0\n",
    "#     min_accuracy = 1\n",
    "#     max_accuracy = 0\n",
    "#     num_exps = 100\n",
    "#     with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "#         train_mnist = pkl.load(file)\n",
    "#         X = train_mnist[0]\n",
    "#         Y = train_mnist[1]\n",
    "#     num_samples = len(X)\n",
    "#     sample_size = 800 if dataset == \"flowers\" else 4000\n",
    "#     for _ in range(num_exps):\n",
    "#         indices = np.random.randint(sample_size, size = sample_size)\n",
    "#         Y_pred = predict(X[indices,:], dataset)\n",
    "#         correct_count = np.sum(Y_pred == Y[indices])\n",
    "#         min_accuracy = min(min_accuracy, correct_count/sample_size)\n",
    "#         max_accuracy = max(max_accuracy, correct_count/sample_size)\n",
    "#         accuracy+=correct_count\n",
    "#     accuracy/= num_exps*sample_size\n",
    "#     print(f\"Accuracy = {accuracy}, Max = {max_accuracy}, Min = {min_accuracy}, dataset = {dataset}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9538\n"
     ]
    }
   ],
   "source": [
    "# with open(\"./data/my_test_x\", \"rb\") as file:\n",
    "#     test_x = pkl.load(file)\n",
    "# with open(\"./data/my_test_y\", \"rb\") as file:\n",
    "#     test_y = pkl.load(file)\n",
    "\n",
    "# y_pred = predict(test_x, \"mnist\")\n",
    "# print(np.sum(y_pred == test_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
